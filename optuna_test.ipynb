{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install SuperSuit\n",
    "!pip install ray[rllib]==0.8.5\n",
    "!pip install lz4\n",
    "!pip install opencv-python==4.5.5.64\n",
    "!pip install dm_tree\n",
    "!pip install stable-baselines3\n",
    "!pip install pettingzoo\n",
    "!pip install sb3-contrib\n",
    "!pip install gym==0.23.1\n",
    "!pip install wandb\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import supersuit as ss\n",
    "\n",
    "from social_dilemmas.envs.pettingzoo_env import MAX_CYCLES\n",
    "from social_dilemmas.envs.pettingzoo_env import parallel_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.ppo import CnnPolicy\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecMonitor\n",
    "\n",
    "num_agents = 2\n",
    "number_of_envs = 16\n",
    "num_cpus=8\n",
    "\n",
    "env = parallel_env(max_cycles=MAX_CYCLES, env = \"harvest\", num_agents = num_agents, proportion=.5)\n",
    "\n",
    "env = ss.resize_v1(env, x_size=36, y_size=36, linear_interp=False)\n",
    "env = ss.frame_stack_v1(env, 1)\n",
    "env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "env = ss.concat_vec_envs_v1(env, number_of_envs, num_cpus=num_cpus, base_class=\"stable_baselines3\")\n",
    "# env = VecMonitor(env, info_keywords=('Utilitarian',), filename='logs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_params(trial:optuna.Trial):\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64, 128, 256, 512])\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
    "    n_steps = trial.suggest_categorical(\"n_steps\", [8, 16, 32, 64, 128, 256, 512, 1024, 2048])\n",
    "    gamma = trial.suggest_categorical(\"gamma\", [0.9, 0.95, 0.98, 0.99, 0.995, 0.999, 0.9999])\n",
    "    ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
    "    return {\"learning_rate\": learning_rate,\n",
    "             \"n_steps\": n_steps,\n",
    "             \"batch_size\": batch_size,\n",
    "             \"gamma\": gamma,\n",
    "             \"ent_coef\": ent_coef\n",
    "            }\n",
    "\n",
    "def calculate_reward(model):\n",
    "    mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=100)\n",
    "    return mean_reward - std_reward\n",
    "\n",
    "\n",
    "class LoggingCallback:\n",
    "    def __init__(self, threshold, trial_number, patience):\n",
    "        self.threshold = threshold\n",
    "        self.trial_number = trial_number\n",
    "        self.patience = patience\n",
    "        self.cb_list = []\n",
    "    def __call__(self, study:optuna.study, frozen_trial: optuna.Trial):\n",
    "        study.set_user_attr('previous_best_value', study.best_value)\n",
    "        if frozen_trial.number > self.trial_number:\n",
    "            previous_best_value = study.user_attrs.get('previous_best_value', None)\n",
    "            if previous_best_value * study.best_value >= 0:\n",
    "                if abs(previous_best_value = study.best_value) < self.threshold:\n",
    "                    self.cb_list.append(frozen_trial.number)\n",
    "                    if len(self.cb_list) > self.patience:\n",
    "                        print('The study stops now..')\n",
    "                        print('With number ', frozen_trial.number, 'and value ', frozen_trial.value)\n",
    "                        print('The previous and current values are {} and {} respectively'\n",
    "                        .format(previous_best_value, study.best_value))\n",
    "                        study.stop()\n",
    "\n",
    "def objective(trial: optuna.Trial):\n",
    "    hyperparameters = ppo_params(trial)\n",
    "    model_ppo = PPO(CnnPolicy, env, *hyperparameters)\n",
    "    model_ppo.learn(total_timesteps=600_000)\n",
    "    reward = calculate_reward(model_ppo)\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study = optuna.create_study(study_name='ppo_study', direction='maximize', sampler = sampler, pruner=optuna.pruners.HyperbandPruner())\n",
    "logging_callback = LoggingCallback(threshold=3, patience=3, trial_number=3)\n",
    "\n",
    "study.optimize(objective, n_trials=30, catch=(ValueError, ), callbacks=[logging_callback])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
