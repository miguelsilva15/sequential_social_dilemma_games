{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imageio\n",
    "!pip install SuperSuit==3.4.0\n",
    "!pip install ray[rllib]==0.8.5\n",
    "!pip install lz4\n",
    "!pip install opencv-python==4.5.5.64\n",
    "!pip install dm_tree\n",
    "!pip install stable-baselines3\n",
    "!pip install pettingzoo==1.18.1\n",
    "!pip install sb3-contrib\n",
    "!pip install gym==0.23.1\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from social_dilemmas.envs.pettingzoo_env import MAX_CYCLES\n",
    "from social_dilemmas.envs.pettingzoo_env import env as aec_env\n",
    "from social_dilemmas.envs.pettingzoo_env import parallel_env\n",
    "from pettingzoo.test import parallel_api_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(env, img, cummulative_reward, step=0, info=\"\", outer_step=0, save=False, path='pictures/near/'):\n",
    "    img.set_data(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d | Reward: %r\" % (env.metadata['name'],step, cummulative_reward))\n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    if save:\n",
    "        name_to_save = path+'step_'+str(outer_step)+'_'+str(step)+'.png'\n",
    "        plt.savefig(name_to_save)\n",
    "        env.render(mode='rgb_array')\n",
    "        return name_to_save\n",
    "    else:\n",
    "        env.render(mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "from random import randint\n",
    "\n",
    "\n",
    "env = aec_env(env = \"harvest\", num_agents = 2, proportion=0.5)\n",
    "env.reset()\n",
    "n_act = env.action_space(env.agents[0]).n\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "counter = 0\n",
    "cummulative_reward = 0\n",
    "for agent in env.agent_iter(101):\n",
    "    counter += 1\n",
    "    obs, reward, done, info = env.last()        \n",
    "    cummulative_reward += reward\n",
    "    action = randint(0,n_act-1) if not done else None\n",
    "    obss = obs.copy()\n",
    "    r = env.step(action)\n",
    "    show_state(env, img, round(cummulative_reward, 2), counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in env.agent_iter(MAX_CYCLES * env.num_agents):\n",
    "    _,_,done,_ = env.last()\n",
    "    action = randint(0,n_act-1) if not done else None\n",
    "    env.step(action)\n",
    "    if not env.agents:\n",
    "        env.reset()\n",
    "# api_test(env, MAX_CYCLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = parallel_env(max_cycles=MAX_CYCLES, env = \"harvest\", num_agents = 2, proportion=.5)\n",
    "env.reset()\n",
    "n_act = env.action_space(env.agents[0]).n\n",
    "for _ in range(MAX_CYCLES * env.num_agents):\n",
    "    actions = {agent: np.random.randint(n_act) for agent in env.agents}\n",
    "    _, _, _, _ = env.step(actions)\n",
    "    if not env.agents:\n",
    "        _ = env.reset()\n",
    "parallel_api_test(env, MAX_CYCLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3.dqn import CnnPolicy, MlpPolicy\n",
    "from stable_baselines3.a2c import CnnPolicy, MlpPolicy\n",
    "# from sb3_contrib.trpo import CnnPolicy, MlpPolicy\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from sb3_contrib import TRPO\n",
    "import supersuit as ss\n",
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecMonitor\n",
    "\n",
    "num_agents = 2\n",
    "number_of_envs = 16\n",
    "num_cpus=8\n",
    "\n",
    "env = parallel_env(max_cycles=MAX_CYCLES, env = \"harvest\", num_agents = num_agents, proportion=.5)\n",
    "\n",
    "# env = ss.color_reduction_v0(env, mode=\"full\")\n",
    "env = ss.resize_v1(env, x_size=36, y_size=36, linear_interp=False)\n",
    "env = ss.frame_stack_v1(env, 1)\n",
    "env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "\n",
    "env = ss.concat_vec_envs_v1(env, number_of_envs, num_cpus=num_cpus, base_class=\"stable_baselines3\")\n",
    "env = VecMonitor(env, info_keywords=('Utilitarian',), filename='logs/')\n",
    "\n",
    "# model = PPO(\n",
    "#     CnnPolicy,\n",
    "#     env,\n",
    "#     verbose=3,\n",
    "#     gamma=.97,\n",
    "#     tensorboard_log=\"runs/A2C\")\n",
    "\n",
    "# model = DQN( \n",
    "#     CnnPolicy,\n",
    "#     env,\n",
    "#     gamma= .95,\n",
    "#     verbose=3,\n",
    "#     exploration_fraction=0.25,\n",
    "#     exploration_final_eps=0.1,\n",
    "#     tensorboard_log=\"runs/Battle_DQN\")\n",
    "model = A2C(\n",
    "    CnnPolicy,\n",
    "    env,\n",
    "    gamma= .98,\n",
    "    ent_coef= 1.6835848102390265e-05,\n",
    "    use_rms_prop= True,\n",
    "    learning_rate= 0.0005932893977209943,\n",
    "    max_grad_norm= 0.8,\n",
    "    vf_coef= 0.47382346265030867,\n",
    "    verbose=3,\n",
    "    tensorboard_log=\"runs/A2C_Fixed_Distance\")\n",
    "# model = TRPO(\n",
    "#     CnnPolicy,\n",
    "#     env,\n",
    "    # gamma= .95,\n",
    "    # ent_coef= 1.0558813779064815e-05,\n",
    "    # learning_rate= 0.0005932893977209943,\n",
    "    # vf_coef= 0.2912291401980419,\n",
    "#     verbose=3,\n",
    "#     tensorboard_log=\"runs/Real_POC_TRPO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    name=\"A2C_Fixed_Distance\",\n",
    "    project=\"sb3\",\n",
    "    config={\"policy_type\": \"CNNPolicy\", \"total_timesteps\": 2_000_000},\n",
    "    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "    monitor_gym=False,  # auto-upload the videos of agents playing the game\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "class UtilitarianCallBack(WandbCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(UtilitarianCallBack, self).__init__(verbose)\n",
    "    def _on_training_start(self):\n",
    "        self._log_freq = 500  # log every 200 calls\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self._log_freq == 0:\n",
    "            counter = 0\n",
    "            uti = sum(item['Utilitarian'] for item in self.locals['infos'])/(number_of_envs*num_agents) # <- number of env running at the same time, number of agents\n",
    "            eq = sum(item['Equality'] for item in self.locals['infos'])/(number_of_envs*num_agents)\n",
    "            # sus = sum(item['Sustainability'] for item in self.locals['infos'])/(number_of_envs*num_agents)\n",
    "            agent_1 = []\n",
    "            agent_2 = []\n",
    "            for numb in range(number_of_envs*num_agents):\n",
    "                if numb%2==0:\n",
    "                    agent_1.append(sum(self.locals['infos'][numb]['Reward']))\n",
    "                else:\n",
    "                    agent_2.append(sum(self.locals['infos'][numb]['Reward']))\n",
    "                    \n",
    "            self.logger.record('custom/utilitarian', uti)\n",
    "            self.logger.record('custom/equality', eq)\n",
    "            self.logger.record('custom/reward_agent_1', np.mean(agent_1))\n",
    "            self.logger.record('custom/reward_agent_2', np.mean(agent_2))\n",
    "            # self.logger.record('custom/sustainability', sus)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miguel/anaconda3/envs/social/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1617/2648240314.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msb3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWandbCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m model.learn(total_timesteps=1000000,     \n\u001b[0m\u001b[1;32m      4\u001b[0m             callback=WandbCallback(\n\u001b[1;32m      5\u001b[0m             \u001b[0mgradient_save_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=2_000_000,     \n",
    "            callback=UtilitarianCallBack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "\n",
    "def save_frames_as_gif(frames, path='./', filename='gym_animation.gif'):\n",
    "    #Mess with this to change frame size\n",
    "    plt.figure(figsize=(frames[0].shape[1], frames[0].shape[0]))\n",
    "\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    anim.save(path + filename, fps=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=100)\n",
    "\n",
    "print(mean_reward, std_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "from random import randint\n",
    "\n",
    "env = aec_env(env = \"harvest\", num_agents = 2, proportion=.5)\n",
    "# env = ss.color_reduction_v0(env, mode=\"full\")\n",
    "env = ss.resize_v1(env, x_size=36, y_size=36, linear_interp=False)\n",
    "env = ss.frame_stack_v1(env, 1)\n",
    "\n",
    "# model = A2C.load(\"policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "name_frames = []\n",
    "\n",
    "outer_counter=0\n",
    "\n",
    "for i in range(10):\n",
    "    outer_counter += 1\n",
    "    env.reset()\n",
    "    img = plt.imshow(env.render(mode='rgb_array'))\n",
    "    counter = 0\n",
    "    cummulative_reward = 0\n",
    "    for agent in env.agent_iter(100):\n",
    "        counter += 1\n",
    "        obs, reward, done, info = env.last()\n",
    "        cummulative_reward += reward\n",
    "        action = model.predict(obs, deterministic=True)[0] if not done else None\n",
    "        env.step(action)\n",
    "        name_frames.append(show_state(env, img, round(cummulative_reward, 3), counter, outer_step=outer_counter\n",
    "        #                               , save=True, path='pictures/far/'\n",
    "                                      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "with imageio.get_writer('far.gif', mode='I') as writer:\n",
    "    for filename in name_frames:\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c54f4e1905cd32010bf341b03818db875e58910116369b81e2031cacd9a8002"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('social')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c54f4e1905cd32010bf341b03818db875e58910116369b81e2031cacd9a8002"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
